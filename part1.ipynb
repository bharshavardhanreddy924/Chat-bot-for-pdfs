{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "import PyPDF2\n",
    "import torch\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt', quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = \"6ab416de-973e-436c-975c-3a3326c5887f\"\n",
    "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
    "INDEX_NAME = \"document-qa\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "GENERATOR_MODEL_NAME = \"facebook/bart-large-cnn\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache_resource\n",
    "def initialize_pinecone():\n",
    "    try:\n",
    "        return Pinecone(api_key=PINECONE_API_KEY)\n",
    "    except Exception as e:\n",
    "        st.error(f\"Failed to initialize Pinecone: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache_resource\n",
    "def load_models():\n",
    "    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    generator = pipeline('text2text-generation', model=GENERATOR_MODEL_NAME, device=0 if torch.cuda.is_available() else -1)\n",
    "    return embedding_model, generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(file):\n",
    "    pdf_reader = PyPDF2.PdfReader(file)\n",
    "    text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(text, chunk_size=1000):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) <= chunk_size:\n",
    "            current_chunk += sentence + \" \"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \" \"\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pinecone_index(pc):\n",
    "    try:\n",
    "        if INDEX_NAME not in pc.list_indexes().names():\n",
    "            pc.create_index(\n",
    "                name=INDEX_NAME,\n",
    "                dimension=384,\n",
    "                metric='cosine',\n",
    "                spec=ServerlessSpec(cloud='aws', region=PINECONE_ENVIRONMENT)\n",
    "            )\n",
    "        return pc.Index(INDEX_NAME)\n",
    "    except Exception as e:\n",
    "        st.error(f\"Failed to create Pinecone index: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def store_in_pinecone(pc, chunks, embeddings):\n",
    "    index = create_pinecone_index(pc)\n",
    "    if index is None:\n",
    "        return\n",
    "    \n",
    "    batch_size = 100\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        i_end = min(i+batch_size, len(chunks))\n",
    "        ids = [str(j) for j in range(i, i_end)]\n",
    "        metadatas = [{\"text\": chunk} for chunk in chunks[i:i_end]]\n",
    "        embeddings_batch = embeddings[i:i_end].tolist()\n",
    "        to_upsert = list(zip(ids, embeddings_batch, metadatas))\n",
    "        try:\n",
    "            index.upsert(vectors=to_upsert)\n",
    "        except Exception as e:\n",
    "            st.error(f\"Failed to upsert vectors: {\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_chunks(pc, query, embedding_model, top_k=3):\n",
    "    try:\n",
    "        index = pc.Index(INDEX_NAME)\n",
    "        query_embedding = embedding_model.encode([query])[0].tolist()\n",
    "        results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "        return [result['metadata']['text'] for result in results['matches']]\n",
    "    except Exception as e:\n",
    "        st.error(f\"Failed to retrieve chunks: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def generate_answer(query, context, generator):\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    try:\n",
    "        response = generator(prompt, max_length=150, num_return_sequences=1, do_sample=False)\n",
    "        return response[0]['generated_text']\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error generating answer: {str(e)}\")\n",
    "        return \"I'm sorry, I couldn't generate an answer. Please try rephrasing your question.\"\n",
    "\n",
    "def qa_bot(pc, query, embedding_model, generator):\n",
    "    try:\n",
    "        relevant_chunks = retrieve_chunks(pc, query, embedding_model)\n",
    "        context = \" \".join(relevant_chunks)\n",
    "        answer = generate_answer(query, context, generator)\n",
    "        return answer, relevant_chunks\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error in QA bot: {str(e)}\")\n",
    "        return \"I'm sorry, an error occurred. Please try again.\", []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(pc, query, embedding_model, generator):\n",
    "    start_time = time.time()\n",
    "    answer, relevant_chunks = qa_bot(pc, query, embedding_model, generator)\n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    return answer, relevant_chunks, processing_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    st.title(\"Document QA Bot\")\n",
    "\n",
    "    pc = initialize_pinecone()\n",
    "    if pc is None:\n",
    "        st.error(\"Failed to initialize Pinecone. Please check your API key and try again.\")\n",
    "        return\n",
    "\n",
    "    embedding_model, generator = load_models()\n",
    "\n",
    "    uploaded_file = st.file_uploader(\"Choose a PDF file\", type=\"pdf\", key=\"pdf_uploader\")\n",
    "\n",
    "    if uploaded_file is not None:\n",
    "        with st.spinner(\"Processing document...\"):\n",
    "            text = process_pdf(uploaded_file)\n",
    "            chunks = split_into_chunks(text)\n",
    "            embeddings = embedding_model.encode(chunks)\n",
    "            store_in_pinecone(pc, chunks, embeddings)\n",
    "        st.success(\"Document processed and stored successfully!\")\n",
    "\n",
    "    st.subheader(\"Ask questions about the document\")\n",
    "    \n",
    "    # Create a text input for the query\n",
    "    query = st.text_input(\"Enter your question:\", key=\"query_input\")\n",
    "\n",
    "    # Create a button to submit the query\n",
    "    if st.button(\"Submit Question\"):\n",
    "        if query:\n",
    "            with st.spinner(\"Generating answer...\"):\n",
    "                answer, relevant_chunks, processing_time = process_query(pc, query, embedding_model, generator)\n",
    "            \n",
    "            st.subheader(\"Answer:\")\n",
    "            st.write(answer)\n",
    "            \n",
    "            st.subheader(\"Processing Time:\")\n",
    "            st.write(f\"{processing_time:.2f} seconds\")\n",
    "            \n",
    "            st.subheader(\"Relevant Document Segments:\")\n",
    "            for i, chunk in enumerate(relevant_chunks):\n",
    "                with st.expander(f\"Segment {i+1}\"):\n",
    "                    st.write(chunk)\n",
    "\n",
    "    # Multiple queries handling\n",
    "    st.subheader(\"Multiple Queries\")\n",
    "    num_queries = st.number_input(\"Number of queries to process:\", min_value=1, max_value=10, value=3)\n",
    "    queries = [st.text_input(f\"Query {i+1}:\") for i in range(num_queries)]\n",
    "    \n",
    "    if st.button(\"Process Multiple Queries\"):\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(executor.map(lambda q: process_query(pc, q, embedding_model, generator), queries))\n",
    "        \n",
    "        for i, (query, (answer, relevant_chunks, processing_time)) in enumerate(zip(queries, results)):\n",
    "            st.subheader(f\"Query {i+1}: {query}\")\n",
    "            st.write(\"Answer:\", answer)\n",
    "            st.write(f\"Processing Time: {processing_time:.2f} seconds\")\n",
    "            with st.expander(\"Relevant Document Segments\"):\n",
    "                for j, chunk in enumerate(relevant_chunks):\n",
    "                    st.write(f\"Segment {j+1}: {chunk}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
